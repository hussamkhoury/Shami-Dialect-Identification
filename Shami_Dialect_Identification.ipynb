{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zil_KUE3v2kO",
    "outputId": "8b3fbfb0-12a6-49f1-c76e-73a3995af391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pOI3XQtfFXU"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a2F9DQPx_KH",
    "outputId": "179bf410-ba84-4ed6-b2dc-d3f8236cf415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37759\n",
      "10829\n",
      "10642\n",
      "7017\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# General list for the whole corpus\n",
    "corpus_tweets = []\n",
    "#Syrian dialect\n",
    "# pandas.read_table for readding .txt files\n",
    "S_data = pd.read_table('/content/drive/My Drive/Colab Notebooks/NLP - Shami dialect Language Identification /psyrian.txt', names=['Text data'])\n",
    "# # add labels to the data\n",
    "S_data['Label'] = 0\n",
    "print(len(S_data))\n",
    "# reducing the size of the Dataframe (take the first 1000 index)\n",
    "S_data = S_data.iloc[:10001,:]\n",
    "# make a list of tuple for the vectorizer\n",
    "for tweet in S_data[\"Text data\"]:\n",
    "  corpus_tweets.append((tweet, 0))\n",
    "\n",
    "#Lebanees dialect\n",
    "L_data = pd.read_table('/content/drive/My Drive/Colab Notebooks/NLP - Shami dialect Language Identification /plebanees.txt', names=['Text data'])\n",
    "L_data['Label'] = 1\n",
    "print(len(L_data))\n",
    "# L_data = L_data.iloc[:5001,:]\n",
    "for tweet in L_data[\"Text data\"]:\n",
    "  corpus_tweets.append((tweet, 1))\n",
    "#Palestinian dialect\n",
    "P_data = pd.read_table('/content/drive/My Drive/Colab Notebooks/NLP - Shami dialect Language Identification /ppalestinian.txt', names=['Text data'])\n",
    "P_data['Label'] = 2\n",
    "print(len(P_data))\n",
    "# P_data = P_data.iloc[:5001,:]\n",
    "for tweet in P_data[\"Text data\"]:\n",
    "  corpus_tweets.append((tweet, 2))\n",
    "\n",
    "#jordinian dialect\n",
    "J_data = pd.read_table('/content/drive/My Drive/Colab Notebooks/NLP - Shami dialect Language Identification /pjordinian.txt', names=['Text data'])\n",
    "J_data['Label'] = 3\n",
    "print(len(J_data))\n",
    "# J_data = J_data.iloc[:5001,:]\n",
    "for tweet in J_data[\"Text data\"]:\n",
    "  corpus_tweets.append((tweet, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfgNTNyBfQJ9"
   },
   "source": [
    "# Splitting and encoding the data using: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gjk67dM2DqEL"
   },
   "source": [
    "TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNC5GigYNsRh"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "# split the data for train and test\n",
    "\n",
    "labels = []\n",
    "tweets = []\n",
    "shuffled_corpus_tweets = random.sample(corpus_tweets, len(corpus_tweets))\n",
    "\n",
    "for item in shuffled_corpus_tweets:\n",
    "  tweets.append(item[0])\n",
    "  labels.append(item[1])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.25, random_state=42) \n",
    "x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42) \n",
    "\n",
    "# making the vectorizer\n",
    "shami_vectorizer = TfidfVectorizer()\n",
    "shami_vectorizer.fit(x_train)\n",
    "x_train_encoded = shami_vectorizer.transform(x_train)\n",
    "x_test_encoded = shami_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsYLDXkWzVYG"
   },
   "source": [
    "Word Emmbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgsmgRR-zbTP"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IPvcj_h3VLM"
   },
   "outputs": [],
   "source": [
    "x_train_tok = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcxbBtFa3zG5"
   },
   "outputs": [],
   "source": [
    "# Padding the sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "x_train_tok = pad_sequences(x_train_tok, padding='post', maxlen=maxlen)\n",
    "x_test_tok = pad_sequences(x_test_tok, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S8ZzQf7fJTd"
   },
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvnuMUxRfbEd"
   },
   "source": [
    "Naive Bayes - GaussianNB -- TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_qGUw2dQEDs"
   },
   "outputs": [],
   "source": [
    "# training a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "GaussianNB_model = GaussianNB().fit(x_train_encoded.toarray() ,y_train)\n",
    "GaussianNB_predictions = GaussianNB_model.predict(x_test_encoded.toarray())\n",
    "\n",
    "\n",
    "\n",
    "dict_ = { 0: \"Syrian\",\n",
    "          1 : \"Lebaneese\",\n",
    "          2 : \"Palestinian\",\n",
    "          3 : \"Jordanian\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBbaoH1jaPqE",
    "outputId": "8802f889-c7c9-41d4-98bf-5bdced12fb9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line model accuracy GaussianNB model TFIDF : 0.668502545983581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.70      2554\n",
      "           1       0.79      0.63      0.70      2652\n",
      "           2       0.68      0.62      0.65      2605\n",
      "           3       0.58      0.61      0.60      1812\n",
      "\n",
      "    accuracy                           0.67      9623\n",
      "   macro avg       0.67      0.66      0.66      9623\n",
      "weighted avg       0.68      0.67      0.67      9623\n",
      "\n",
      "Text :  هاد فرح عم يمر من جنب المشفى حبيبي ماني سامعك بحكي معك من تروح الضجه, \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 0\n",
      "Text : مصاري اناكلت عليك بترجعلك ضعف , \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 3\n",
      "Text :  عهد ابو خشبه ما يتحاسب , \n",
      "Predicted_Label : Lebaneese, \n",
      "Real_label : 1\n",
      "Text : و اختها عالسريع ردت اله يعينك فيما ابتلاك يخوي ي حبيبي ي غالي , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 3\n",
      "Text :  ماشا اله ماعاد فيه اطفال خلاص , \n",
      "Predicted_Label : Lebaneese, \n",
      "Real_label : 1\n",
      "Text :  شكلك حساسه م تدى مشى شكلو يمزح اكيد م تنهمى وبدك ترتاحى اكتر , \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 2\n",
      "Text :  اولا انا متزوجه وزوجي بحبني كتير وانا مو مقصره معه بشي ولاشي في بنت كانت الها قصه مع شب وزوجي طلب يدخل بالموضوع واخد رقمها مني وحكالي واله ماراح اكوم بالصوره ومرت شهور ع القصه فجاه اجت اخت البنت خبرتني انه اختها الها علاقه بزوجي وبكلمه بعض واتس وحب وغرام ولما كشفت الموضوع خبرها تنكر وهي الان بتنكر وبتحلف ع المصحف هي مطلقه ومعها ولدين انا حردانه عند , \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 2\n",
      "Text : بس طول الست اشهر و انا كل ليله احلم اني اركض و هي تركض وراي اركض و هي تركض وراي , \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 3\n",
      "Text :  لا حول ولاقوه الا باله بيعطيكي ياعمي مصروف لالك ولعبداله, \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 0\n",
      "Text :  روحي ع الدكتور عمر المناصره بالخدمه دكتور راع وان شا اله بتستفيدي , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "GaussianNB_model_accuracy = GaussianNB_model.score(x_test_encoded.toarray() , y_test)\n",
    "print(f\"Base line model accuracy GaussianNB model TFIDF : {GaussianNB_model_accuracy}\")\n",
    "# print(f\"f1_score: {f1_score(y_test, GaussianNB_predictions, average='macro')}\")\n",
    "# print(f\"precision_score: {precision_score(y_test, GaussianNB_predictions, average='macro')}\")\n",
    "# print(f\"recall_score: {recall_score(y_test, GaussianNB_predictions, average='macro')}\") \n",
    "\n",
    "print(classification_report(y_test, GaussianNB_predictions))    \n",
    "for item in range(10):\n",
    "  index = GaussianNB_predictions[item]\n",
    "  print(f\"Text : {x_test[item]}, \\nPredicted_Label : {dict_[index]}, \\nReal_label : {y_test[item]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM28HWzkm81w"
   },
   "source": [
    "Naive Bayes - GaussianNB -- Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGzMXUfNm8-V"
   },
   "outputs": [],
   "source": [
    "GaussianNB_model = GaussianNB().fit(x_train_tok, y_train)\n",
    "GaussianNB_predictions = GaussianNB_model.predict(x_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW_Ry4IUtD2q",
    "outputId": "822a3903-295e-47f8-8090-5418be120a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line model accuracy GaussianNB model word embedding : 0.2989712147978801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.00      0.01      2554\n",
      "           1       0.29      1.00      0.45      2652\n",
      "           2       0.39      0.06      0.10      2605\n",
      "           3       0.34      0.04      0.07      1812\n",
      "\n",
      "    accuracy                           0.30      9623\n",
      "   macro avg       0.33      0.27      0.16      9623\n",
      "weighted avg       0.33      0.30      0.17      9623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB_model_accuracy = GaussianNB_model.score(x_test_tok , y_test)\n",
    "# print(f\"Base line model accuracy : {GaussianNB_model_accuracy}\")\n",
    "# print(f\"f1_score: {f1_score(y_test, GaussianNB_predictions, average='macro')}\")\n",
    "# print(f\"precision_score: {precision_score(y_test, GaussianNB_predictions, average='macro')}\")\n",
    "# print(f\"recall_score: {recall_score(y_test, GaussianNB_predictions, average='macro')}\")\n",
    "# print(precision_recall_fscore_support(y_test, GaussianNB_predictions))  \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "GaussianNB_model_accuracy_word_embedding = GaussianNB_model.score(x_test_tok , y_test)\n",
    "print(f\"Base line model accuracy GaussianNB model word embedding : {GaussianNB_model_accuracy_word_embedding}\")\n",
    "print(classification_report(y_test, GaussianNB_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ngp8APEgc1F"
   },
   "source": [
    "OneVsOneClassifier -- TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaPGiEQKgKTB"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "OneVsOneClassifier_model = OneVsOneClassifier(LinearSVC(random_state=1))\n",
    "OneVsOneClassifier_model.fit(x_train_encoded.toarray() ,y_train)\n",
    "OneVsOneClassifier_model_predictions = OneVsOneClassifier_model.predict(x_test_encoded.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTkwCwMogyY5",
    "outputId": "56ec0123-9fc5-4afe-b37b-b999049113de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line model accuracy : 0.8147147459212304\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      2554\n",
      "           1       0.80      0.86      0.83      2652\n",
      "           2       0.80      0.80      0.80      2605\n",
      "           3       0.77      0.69      0.73      1812\n",
      "\n",
      "    accuracy                           0.81      9623\n",
      "   macro avg       0.81      0.81      0.81      9623\n",
      "weighted avg       0.81      0.81      0.81      9623\n",
      "\n",
      "Text :  هاد فرح عم يمر من جنب المشفى حبيبي ماني سامعك بحكي معك من تروح الضجه, \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 0\n",
      "Text : مصاري اناكلت عليك بترجعلك ضعف , \n",
      "Predicted_Label : Jordanian, \n",
      "Real_label : 3\n",
      "Text :  عهد ابو خشبه ما يتحاسب , \n",
      "Predicted_Label : Lebaneese, \n",
      "Real_label : 1\n",
      "Text : و اختها عالسريع ردت اله يعينك فيما ابتلاك يخوي ي حبيبي ي غالي , \n",
      "Predicted_Label : Jordanian, \n",
      "Real_label : 3\n",
      "Text :  ماشا اله ماعاد فيه اطفال خلاص , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 1\n",
      "Text :  شكلك حساسه م تدى مشى شكلو يمزح اكيد م تنهمى وبدك ترتاحى اكتر , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 2\n",
      "Text :  اولا انا متزوجه وزوجي بحبني كتير وانا مو مقصره معه بشي ولاشي في بنت كانت الها قصه مع شب وزوجي طلب يدخل بالموضوع واخد رقمها مني وحكالي واله ماراح اكوم بالصوره ومرت شهور ع القصه فجاه اجت اخت البنت خبرتني انه اختها الها علاقه بزوجي وبكلمه بعض واتس وحب وغرام ولما كشفت الموضوع خبرها تنكر وهي الان بتنكر وبتحلف ع المصحف هي مطلقه ومعها ولدين انا حردانه عند , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 2\n",
      "Text : بس طول الست اشهر و انا كل ليله احلم اني اركض و هي تركض وراي اركض و هي تركض وراي , \n",
      "Predicted_Label : Jordanian, \n",
      "Real_label : 3\n",
      "Text :  لا حول ولاقوه الا باله بيعطيكي ياعمي مصروف لالك ولعبداله, \n",
      "Predicted_Label : Syrian, \n",
      "Real_label : 0\n",
      "Text :  روحي ع الدكتور عمر المناصره بالخدمه دكتور راع وان شا اله بتستفيدي , \n",
      "Predicted_Label : Palestinian, \n",
      "Real_label : 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "OneVsOneClassifier_model_accuracy = OneVsOneClassifier_model.score(x_test_encoded.toarray() , y_test)\n",
    "print(f\"Base line model accuracy : {OneVsOneClassifier_model_accuracy}\")\n",
    "# print(f\"f1_score: {f1_score(y_test, OneVsOneClassifier_model_predictions, average='macro')}\")\n",
    "# print(f\"precision_score: {precision_score(y_test, OneVsOneClassifier_model_predictions, average='macro')}\")\n",
    "# print(f\"recall_score: {recall_score(y_test, OneVsOneClassifier_model_predictions, average='macro')}\")   \n",
    "\n",
    "print(classification_report(y_test, OneVsOneClassifier_model_predictions))\n",
    "for item in range(10):\n",
    "  index = OneVsOneClassifier_model_predictions[item]\n",
    "  print(f\"Text : {x_test[item]}, \\nPredicted_Label : {dict_[index]}, \\nReal_label : {y_test[item]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwWs8p_RwiQO"
   },
   "source": [
    "OneVsOneClassifier -- Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptux9Vc1yxCH",
    "outputId": "c2fcc515-8b8b-40d2-bbc7-e9307d349d26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "OneVsOneClassifier_model = OneVsOneClassifier(LinearSVC(random_state=1))\n",
    "OneVsOneClassifier_model.fit(x_train_tok, y_train)\n",
    "OneVsOneClassifier_model_predictions = OneVsOneClassifier_model.predict(x_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bE33Iy9fzKcn",
    "outputId": "31fb959f-08fb-4eb1-eb8e-473d61fef6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base line model accuracy : 0.2840070664034085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.04      0.07      2554\n",
      "           1       0.32      0.64      0.42      2652\n",
      "           2       0.29      0.16      0.21      2605\n",
      "           3       0.21      0.29      0.24      1812\n",
      "\n",
      "    accuracy                           0.28      9623\n",
      "   macro avg       0.28      0.28      0.24      9623\n",
      "weighted avg       0.28      0.28      0.24      9623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "OneVsOneClassifier_model_accuracy = OneVsOneClassifier_model.score(x_test_tok , y_test)\n",
    "print(f\"Base line model accuracy : {OneVsOneClassifier_model_accuracy}\")\n",
    "OneVsOneClassifier_model_accuracy = OneVsOneClassifier_model.score(x_test_tok , y_test)\n",
    "# classification report\n",
    "print(classification_report(y_test, OneVsOneClassifier_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1L3vtrwIqdx"
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXjsAlHrj_74"
   },
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAQGCTrIfBt7",
    "outputId": "3da0c932-e938-46bf-e2ce-820eeeacde87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          6590600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 54)                27648     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                550       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 6,618,853\n",
      "Trainable params: 6,618,853\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding, Bidirectional, Dropout\n",
    "\n",
    "Embedding_dim = 100\n",
    "#23683\n",
    "model = Sequential()\n",
    "# model.add(layers.Embedding(vocab_size, Embedding_dim, input_length=maxlen))\n",
    "# InputLayer( input_shape=(img_width, img_heigh, img_ch)) #input_tensor=img1,\n",
    "# model.add(layers.Embedding(len(shami_vectorizer.get_feature_names()), Embedding_dim, input_length=24010)) #tfidf\n",
    "model.add(layers.Embedding(vocab_size, Embedding_dim, input_length=maxlen)) # Embedding\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(len(x_train[0]))))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bm-hELvfkIg5",
    "outputId": "c9f4d6db-2caa-4ce8-f265-100ddcec26ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 65878).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 65878).\n",
      "   3/2887 [..............................] - ETA: 307:43:39 - loss: 1.6095 - accuracy: 0.2778"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "filepath = \"/content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "history = model.fit(x_train_encoded.toarray(), np.asarray(y_train),\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(x_test_encoded.toarray(), np.asarray(y_test)),\n",
    "                    batch_size=10,\n",
    "                    callbacks=desired_callbacks)\n",
    "train_loss, train_accuracy = model.evaluate(x_train_encoded.toarray(), np.asarray(y_train).astype(np.float32), verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(x_test_encoded.toarray(), np.asarray(y_test).astype(np.float32), verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j1gyL5cLdR2"
   },
   "source": [
    "Tokenize for the train - validation - test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuuJV5-5Eqi0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1rsAyPxEvZS"
   },
   "outputs": [],
   "source": [
    "x_train_tok = tokenizer.texts_to_sequences(x_train_)\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hBbS3mGEx0Q"
   },
   "outputs": [],
   "source": [
    "# Padding the sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "x_train_tok_ = pad_sequences(x_train_tok, padding='post', maxlen=maxlen)\n",
    "x_val_tok_ = pad_sequences(x_val, padding='post', maxlen=maxlen)\n",
    "x_test_tok = pad_sequences(x_test_tok, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XPpDJy_68gN"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding,Dropout,SpatialDropout1D,Conv1D,MaxPooling1D,GRU,BatchNormalization,LeakyReLU\n",
    "from tensorflow.python.keras import regularizers\n",
    "Embedding_dim = 100\n",
    "#23683\n",
    "filters = 7\n",
    "kernel_size = (7,)\n",
    "lstm_units = 100\n",
    "# word_index = tokenizer.word_index\n",
    "# text_embedding = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Embedding(input_dim=text_embedding.shape[0],output_dim=text_embedding.shape[1],\n",
    "                    # weights=[text_embedding],input_length=maxlen,trainable=True))\n",
    "model.add(layers.Embedding(vocab_size, Embedding_dim, input_length=maxlen))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters, kernel_size=kernel_size,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(lstm_units,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters, kernel_size=kernel_size,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(lstm_units,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Conv1D(filters, kernel_size=kernel_size,kernel_regularizer=regularizers.l2(0.00001), padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(lstm_units,dropout=0.5, recurrent_dropout=0.5)))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD4ALcpuOAh_",
    "outputId": "b2a4db91-2544-4c4c-f970-e75b5ad704c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "2165/2165 [==============================] - 335s 149ms/step - loss: 1.2794 - accuracy: 0.3465 - val_loss: 0.9493 - val_accuracy: 0.6044\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.16261, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 2/6\n",
      "2165/2165 [==============================] - 344s 159ms/step - loss: 0.8849 - accuracy: 0.6376 - val_loss: 0.7250 - val_accuracy: 0.7245\n",
      "\n",
      "Epoch 00002: loss improved from 1.16261 to 0.84517, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 3/6\n",
      "2165/2165 [==============================] - 370s 171ms/step - loss: 0.6580 - accuracy: 0.7596 - val_loss: 0.6735 - val_accuracy: 0.7560\n",
      "\n",
      "Epoch 00003: loss improved from 0.84517 to 0.65526, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 4/6\n",
      "2165/2165 [==============================] - 364s 168ms/step - loss: 0.5482 - accuracy: 0.8067 - val_loss: 0.6345 - val_accuracy: 0.7740\n",
      "\n",
      "Epoch 00004: loss improved from 0.65526 to 0.55201, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 5/6\n",
      "2165/2165 [==============================] - 360s 166ms/step - loss: 0.4815 - accuracy: 0.8372 - val_loss: 0.6829 - val_accuracy: 0.7813\n",
      "\n",
      "Epoch 00005: loss improved from 0.55201 to 0.49183, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 6/6\n",
      "2165/2165 [==============================] - 325s 150ms/step - loss: 0.4343 - accuracy: 0.8529 - val_loss: 0.6447 - val_accuracy: 0.7789\n",
      "\n",
      "Epoch 00006: loss improved from 0.49183 to 0.45354, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Training Accuracy: 0.8987\n",
      "Testing Accuracy:  0.7801\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "filepath = \"/content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "history = model.fit(x_train_tok_, np.asarray(y_train_),\n",
    "                    epochs=6,\n",
    "                    verbose=True,\n",
    "                    validation_data=(x_val_tok_, np.asarray(y_val)),\n",
    "                    batch_size=10,\n",
    "                    callbacks=desired_callbacks)\n",
    "train_loss, train_accuracy = model.evaluate(x_train_tok_, np.asarray(y_train_).astype(np.float32), verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(x_test_tok, np.asarray(y_test).astype(np.float32), verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_x0iRUeQziP",
    "outputId": "a07f7c22-8754-40c8-f29f-62e3fc1333ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2165/2165 [==============================] - 338s 152ms/step - loss: 0.4238 - accuracy: 0.8568 - val_loss: 0.7028 - val_accuracy: 0.7811\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.42378, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 2/3\n",
      "2165/2165 [==============================] - 334s 154ms/step - loss: 0.4106 - accuracy: 0.8630 - val_loss: 0.6785 - val_accuracy: 0.7865\n",
      "\n",
      "Epoch 00002: loss improved from 0.42378 to 0.41057, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Epoch 3/3\n",
      "2165/2165 [==============================] - 330s 152ms/step - loss: 0.3785 - accuracy: 0.8735 - val_loss: 0.7084 - val_accuracy: 0.7829\n",
      "\n",
      "Epoch 00003: loss improved from 0.41057 to 0.37847, saving model to /content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\n",
      "Training Accuracy: 0.9173\n",
      "Testing Accuracy:  0.7766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      2460\n",
      "           1       0.73      0.84      0.78      2710\n",
      "           2       0.80      0.74      0.77      2668\n",
      "           3       0.71      0.68      0.69      1785\n",
      "\n",
      "    accuracy                           0.78      9623\n",
      "   macro avg       0.78      0.77      0.77      9623\n",
      "weighted avg       0.78      0.78      0.78      9623\n",
      "\n",
      "(array([0.85988894, 0.73397746, 0.79951397, 0.70667447]), array([0.81829268, 0.84095941, 0.73988006, 0.67619048]), array([0.8385753 , 0.78383491, 0.76854195, 0.69109648]), array([2460, 2710, 2668, 1785]))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "emb_model = load_model(\"/content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\")\n",
    "filepath = \"/content/drive/MyDrive/Colab Notebooks/NLP - Shami dialect Language Identification /LanguageModel.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n",
    "history = emb_model.fit(x_train_tok_, np.asarray(y_train_),\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(x_val_tok_, np.asarray(y_val)),\n",
    "                    batch_size=10,\n",
    "                    callbacks=desired_callbacks)\n",
    "train_loss, train_accuracy = emb_model.evaluate(x_train_tok_, np.asarray(y_train_).astype(np.float32), verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = emb_model.evaluate(x_test_tok, np.asarray(y_test).astype(np.float32), verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "y_pred1 = emb_model.predict(x_test_tok)\n",
    "pred = []\n",
    "for item in y_pred1:\n",
    "  pred.append(np.argmax(item))\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "print(precision_recall_fscore_support(y_test, pred))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJStauAauIJ-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Shami Dialect Identification_extended_RAM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
